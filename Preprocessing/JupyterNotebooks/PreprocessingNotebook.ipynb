{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any calculation, just have a look at your data. Even with cured data sets, the answer to one or more of the following questions could be \"yes\":\n",
    "- Is there anything wrong with the data?\n",
    "- Are there any quirks with the data?\n",
    "- Do I need to fix or remove any of the data?\n",
    "\n",
    "Let's read some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "init_data = pd.read_csv('init_data.csv')\n",
    "init_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection (we see the first 30 and the last 30), we can detect some possible quirks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- v5 seems to be constant\n",
    "- v6 seems to be -v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to confirm the suspicions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed\n",
    "- v5 has null std, thus constant. We remove it.\n",
    "- v6 and v1 have opposite mean, the same std, and opposite-crossed min, max and quartiles. We check whether v6 is equal to -v1. If so, we remove v6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(init_data.v6.values) == list(init_data.v1.values * (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.drop('v5', axis=1, inplace=True)\n",
    "init_data.drop('v6', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else? What about v4 and v7? \n",
    "A picture is worth a thousand words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(init_data, hue='c', vars=['v1', 'v2', 'v3', 'v4', 'v7'], size=2.5, palette=None);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship with v1, v2, and v3 of both v4 and v7 is the same, but for the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that v7 is equal to v4+2.5. Let's check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(init_data.v7.values) == list(init_data.v4.values + 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove v7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.drop('v7', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set, as it is now, is a famous data set used by Fisher in 1936, called Iris.\n",
    "The variables v1 to v4 correspond to measurements of sepal length, sepal width, petal length, and petal width of three species of Iris plants (Iris Setosa, Iris Versicolor, and Iris Virginica; corresponding to 1, 2 and 3 in our variable c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = init_data\n",
    "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(iris, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5, palette=None);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is coceived for trying to determine the species of an Iris plant, using the four measurements. Therefore, it is a classification problem with three classes, corresponding to the three species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Setosa is separable from the rest just by looking at petal lengths or widths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, it does not seem obvious for Versicolor and Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have a look at outliers and missing values. We consider the following (naÃ¯ve) approaches:\n",
    "- Outliers: Remove rows containing outliers. We will consider both individual and colective  outlier detections.\n",
    "- Missing values (nan): Imputation using the mean value, or row removal. \"nan\" stands for \"not a number\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open question 1: Which one would you perform first?\n",
    "- Outliers + nan? => Removing a row because of an outlier affects the mean of all columns, not only the one containing it, for the subsequent nan treatment. We assume that nan are ignored in the mean and std initial calculation.\n",
    "- nan + outliers? => The imputation using the mean does not affect the posterior mean, but affects the std."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens in our Iris data in both outliers and missing values treatments separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have artificially introduced some nan in petal length and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = pd.read_csv('nan_data.csv')\n",
    "nan_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "nan_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the count for petal length and width we can see that there are 6 nan in each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean')\n",
    "nan_imputed_data = pd.DataFrame(data=imp.fit_transform(nan_data))\n",
    "nan_imputed_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "nan_imputed_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the plots look now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_imputed_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(nan_imputed_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious separation is now possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had just ignored those rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_removed_data = nan_data\n",
    "nan_removed_data.dropna(axis=0, how='any', inplace=True)\n",
    "nan_removed_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_removed_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(nan_removed_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the nan have been artificially introduced by us, we will not further use nan_removed_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check colectively, using Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "outlier_data = pd.read_csv('iris_data.csv')\n",
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "elip_env = sklearn.covariance.EllipticEnvelope().fit(outlier_data)\n",
    "detection = elip_env.predict(outlier_data)\n",
    "outlier_positions_mah = [x for x in range(outlier_data.shape[0]) if detection[x] == -1]\n",
    "if detection is []:\n",
    "    print(\"There are not outliers in the data.\")\n",
    "else:\n",
    "    print(\"The outliers found are in positions:\\n\" + str(outlier_positions_mah))\n",
    "    classes_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    print(\"They correspond respectively to classes:\\n\" +\n",
    "          str([classes_names[x-1] for x in outlier_data.species.values[outlier_positions_mah]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.species.values[outlier_positions_mah] += 3\n",
    "outlier_data.species.replace([1, 2, 3, 4, 5, 6], \n",
    "                             ['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa',\n",
    "                              'Outliers Versicolor', 'Outliers Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a single variable contributing alone to the colective assertion as outliers of those 15 rows.\n",
    "\n",
    "Why? Think on a 3D-ellipsoid whose axes's lengths are a, b, and c, inside a box whose measures are also a, b, and c.\n",
    "A point in one corner of the box is inside the box, but outside the ellipsoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check individualy, i.e. variable by variable. We opt for the robust option based on boxplots, i.e.\n",
    "$$x \\in X \\:\\: outlier \\:\\:\\Leftrightarrow \\:\\: x\\notin \\left[Q_1 - 1.5 * IQR, Q_3 + 1.5 * IQR\\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa', 'Outliers Versicolor', 'Outliers Virginica'], [1, 2, 3, 1, 2, 3], inplace=True)\n",
    "ax = sns.boxplot(data=outlier_data[outlier_data.columns[:-1]], orient=\"h\", palette=\"Set2\", linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look to sepal width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.boxplot(y=\"sepal_width\", data=outlier_data, orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = outlier_data.describe()[\"sepal_width\"][\"75%\"] - outlier_data.describe()[\"sepal_width\"][\"25%\"]\n",
    "whiskers = [outlier_data.describe()[\"sepal_width\"][\"25%\"] - (1.5 * IQR), outlier_data.describe()[\"sepal_width\"][\"75%\"] + (1.5 * IQR)]\n",
    "outlier_positions_box = [x for x in range(outlier_data.shape[0]) if outlier_data.sepal_width.values[x] < whiskers[0] or outlier_data.sepal_width.values[x] > whiskers[1]]\n",
    "print(\"The outliers found are in positions:\\n\" + str(outlier_positions_box))\n",
    "print(\"They correspond respectively to sepal widths:\\n\" + str(outlier_data.sepal_width.values[outlier_positions_box]))\n",
    "classes_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "print(\"They correspond respectively to classes:\\n\" + str([classes_names[x-1] for x in outlier_data.species.values[outlier_positions_box]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.species.values[outlier_positions_box] += 3\n",
    "outlier_data.species.replace([1, 2, 3, 4, 5, 6], \n",
    "                             ['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa',\n",
    "                              'Outliers Versicolor', 'Outliers Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open question 2: Which outlier treatment order would you choose?:\n",
    "- First individual, then colective? => Then the ellipsoid for collective changes after individual.\n",
    "- First colective, then individual? => Then the boxplots change after collective.\n",
    "- Both in parallel? => Then present outliers affect both the ellipsoid and the boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do it here in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa', 'Outliers Versicolor', 'Outliers Virginica'], [1, 2, 3, 1, 2, 3], inplace=True)\n",
    "outlier_free_data = outlier_data\n",
    "outlier_positions = list(np.sort(outlier_positions_mah + outlier_positions_box))\n",
    "outlier_free_data.drop(outlier_free_data.index[outlier_positions], inplace=True)\n",
    "outlier_free_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_free_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_free_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], size=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now all three classes are almost already separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applying the Fayyad-Irani MDLP discretization algorithm, there is not an implementation in any standard Python package. Therefore, we have a perfect excuse to use Orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, a note about data csv files format in Orange.\n",
    "\n",
    "Notice that a usual csv file looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sepal_length,sepal_width,petal_length,petal_width,species\n",
    "\n",
    "7.7,2.8,6.7,2.0,Virginica\n",
    "\n",
    "6.0,2.2,4.0,1.0,Versicolor\n",
    "\n",
    "6.6,3.0,4.4,1.4,Versicolor\n",
    "\n",
    "6.7,3.3,5.7,2.1,Virginica\n",
    "\n",
    "4.9,3.1,1.5,0.1,Setosa\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Orange you need to add 2 extra rows containing info about\n",
    "- First the \"type of data\" in each attribute (\"c\" for \"continuous\", \"d\" for \"discrete\", and \"s\" for \"string\").\n",
    "- Second the \"attribute kind\" (\"class\" in the attribute of interest, and \"meta\" in the \"metadata\", i.e. attributes providing some extra information like, for instance, an index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sepal_length,sepal_width,petal_length,petal_width,species\n",
    "\n",
    "c,c,c,c,d\n",
    "\n",
    ", , , ,class\n",
    "\n",
    "7.7,2.8,6.7,2.0,Virginica\n",
    "\n",
    "6.0,2.2,4.0,1.0,Versicolor\n",
    "\n",
    "6.6,3.0,4.4,1.4,Versicolor\n",
    "\n",
    "6.7,3.3,5.7,2.1,Virginica\n",
    "\n",
    "4.9,3.1,1.5,0.1,Setosa\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is not a goal by itself. Its aim is to prepare the data for a posterior task (learning). The task we will choose is a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "print(\"Some data\")\n",
    "display(Image(filename='DecisionTreeSampleData.png'))\n",
    "\n",
    "print(\"Decision tree levels\")\n",
    "display(Image(filename='DecisionTreeLevels.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of our four Iris datasets:\n",
    "- Raw data\n",
    "- Raw discretized data\n",
    "- Cleaned data\n",
    "- Cleaned discretized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orange screen shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='OrangeScreenShot1.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore principal component analysis (PCA). We will see how many principal components (PCs) are selected when we want to capture at least 95% of the total variance, as well as the linear combinations defining them. We do not need to perform mean centering because PCA from Scikit-Learn does it internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First with raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('iris_data.csv')\n",
    "raw_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "X_raw = raw_data[raw_data.columns[:-1]]\n",
    "pca.fit(X_raw)\n",
    "X_reduced_raw = pca.transform(X_raw)\n",
    "raw_pca_data = pd.DataFrame(data=X_reduced_raw, columns=[\"PC1\", \"PC2\"])\n",
    "raw_pca_data = pd.concat([raw_pca_data, raw_data[raw_data.columns[-1]]], axis=1)\n",
    "print(\"There have been selected \" + str(X_reduced_raw.shape[1]) + \" principal components.\")\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('iris_clean_data.csv')\n",
    "clean_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "X_clean = clean_data[clean_data.columns[:-1]]\n",
    "pca.fit(X_clean)\n",
    "X_reduced_clean = pca.transform(X_clean)\n",
    "clean_pca_data = pd.DataFrame(data=X_reduced_clean, columns=[\"PC1\", \"PC2\"])\n",
    "clean_pca_data = pd.concat([clean_pca_data, clean_data[clean_data.columns[-1]]], axis=1)\n",
    "print(\"There have been selected \" + str(X_reduced_clean.shape[1]) + \" principal components.\")\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare both by means of scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=(16, 6), ncols=2)\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.3)\n",
    "y_raw = raw_data.species.values\n",
    "y_clean = clean_data.species.values\n",
    "ax1.scatter(X_reduced_raw[:, 0], X_reduced_raw[:, 1], c=y_raw, alpha = 1.0)\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax2.scatter(X_reduced_clean[:, 0], X_reduced_clean[:, 1], c=y_clean, alpha = 1.0)\n",
    "ax2.set_title('Clean data')\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expect better performance with the clean data, at least for low depth levels. We check it in Orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orange screen shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='OrangeScreenShot2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have only 4 features, there are only 6 posible subsets of size 2. We consider all 6 2-features subsets in the clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: Which pair do you think will work the best? Better than PCs? Better than the original 4-features data?\n",
    "\n",
    "Answer: It depends on the algorithm you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right questions: Which one do you think will work best with decision trees? Better than PCs? Better than the original 4-features data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a reminder of the 2-feature pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='FSS2Example.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare them with PCA and also with the original clean data in Orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orange screen shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='OrangeScreenShot3.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a powerful package written in Python and developed by part of the developers of Scikit-Learn, called Imbalanced-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is developed through GitHub (see https://github.com/scikit-learn-contrib/imbalanced-learn), and there is also an official website (see http://imbalanced-learn.org/en/stable/) where you can find all the info you might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I strongly recommend to read the user guide (see http://imbalanced-learn.org/en/stable/user_guide.html) as well as the general examples as a complement to it (see http://imbalanced-learn.org/en/stable/auto_examples/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
